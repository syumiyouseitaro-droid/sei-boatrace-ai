import streamlit as st
import pandas as pd
import numpy as np
import requests
import re
from bs4 import BeautifulSoup
import unicodedata
import pickle
import warnings
warnings.filterwarnings('ignore')

# åŸºæœ¬è¨­å®šï¼ˆå°¼å´å›ºå®šï¼‰
JCD = "13"

# --- ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="ç«¶è‰‡AIäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå°¼å´ï¼‰", page_icon="ğŸš¤", layout="centered")
st.title("ç«¶è‰‡AIäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå°¼å´å°‚ç”¨ï¼‰")
st.markdown("æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã¨AIï¼ˆHistGradientBoostingï¼‰ã‚’ç”¨ã„ã¦ã€3é€£å˜ã®äºˆæƒ³ãƒˆãƒƒãƒ—30ä»¥å†…ã‚’ç®—å‡ºã—ã¾ã™ã€‚")

# --- ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ ---
# ãƒ¢ãƒ‡ãƒ«ã‚„ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã¯é‡ã„ãŸã‚ã€ä¸€åº¦ã ã‘èª­ã¿è¾¼ã‚“ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆä¿å­˜ï¼‰ã—ã¾ã™
@st.cache_resource
def load_models():
    clf_top3 = pickle.load(open("model_top3.pkl", "rb"))
    clf_1st = pickle.load(open("model_1st.pkl", "rb"))
    clf_2nd = pickle.load(open("model_2nd.pkl", "rb"))
    features = pickle.load(open("model_features.pkl", "rb"))
    return clf_top3, clf_1st, clf_2nd, features

@st.cache_data
def load_and_preprocess_boatracer():
    boatracer_df = pd.read_csv("boatracer.data.csv", header=1)
    
    def clean_pct(val):
        if pd.isna(val): return np.nan
        val_str = str(val).replace('%', '').strip()
        if val_str in ['- -', '-', '']: return np.nan
        try: return float(val_str)
        except ValueError: return np.nan
        
    def clean_st(val):
        if pd.isna(val): return np.nan
        val_str = str(val).strip()
        if val_str in ['- -', '-', '']: return np.nan
        try: return float(val_str)
        except ValueError: return np.nan

    pct_cols = ['3é€£å¯¾ç‡(%)', '1ç€ç‡(%)', '2ç€ç‡(%)', '3ç€ç‡(%)']
    st_cols = ['å¹³å‡ST', 'å¹³å‡ã‚¹ã‚¿ãƒ¼ãƒˆé †']
    
    for col in pct_cols: boatracer_df[col] = boatracer_df[col].apply(clean_pct)
    for col in st_cols: boatracer_df[col] = boatracer_df[col].apply(clean_st)
    return boatracer_df

@st.cache_data
def load_race_data():
    return pd.read_csv("race.data.csv", header=1)


# --- ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•° ---
def normalize_text(text):
    if not text: return ""
    return unicodedata.normalize('NFKC', text).replace(" ", "").replace("ã€€", "").strip()

def scrape_target_race_basic(hd, rno):
    session = requests.Session()
    session.headers.update({"User-Agent": "Mozilla/5.0"})
    params = {"rno": str(rno), "jcd": JCD, "hd": hd}

    url_list = "https://www.boatrace.jp/owpc/pc/race/racelist"
    res_list = session.get(url_list, params=params, timeout=15)
    res_list.encoding = res_list.apparent_encoding
    soup_list = BeautifulSoup(res_list.text, "html.parser")
    
    if not soup_list or not soup_list.select("tbody.is-fs12"):
        raise ValueError("å‡ºèµ°è¡¨ã®ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã€‚æ—¥ç¨‹ã¨ãƒ¬ãƒ¼ã‚¹ç•ªå·ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    def parse_values(td_element):
        if not td_element: return [np.nan, np.nan, np.nan]
        raw_texts = td_element.get_text(separator="|").split("|")
        vals = []
        for t in raw_texts:
            clean_t = t.strip().replace('%', '')
            if re.match(r'^[0-9.]+$', clean_t): vals.append(float(clean_t))
        while len(vals) < 3: vals.append(np.nan)
        return vals[:3]

    racers_info = {}
    for tbody in soup_list.select("tbody.is-fs12"):
        waku_td = tbody.select_one("td[class*='is-boatColor']")
        if not waku_td: continue
        waku = normalize_text(waku_td.get_text(strip=True))
        
        reg_info = tbody.select_one(".is-fs11").get_text(strip=True)
        reg_match = re.search(r'\d{4}', reg_info)
        
        if waku.isdigit() and reg_match:
            stats_tds = tbody.select("td.is-lineH2")
            nat = parse_values(stats_tds[1]) if len(stats_tds) > 1 else [np.nan]*3
            mot = parse_values(stats_tds[3]) if len(stats_tds) > 3 else [np.nan]*3
            
            racers_info[int(waku)] = {
                "ç™»éŒ²ç•ªå·": int(reg_match.group()),
                "å…¨å›½å‹ç‡": nat[0],
                "å…¨å›½3é€£": nat[2],
                "ãƒ¢ãƒ¼ã‚¿ãƒ¼3é€£": mot[2],
                "å±•ç¤ºã‚¿ã‚¤ãƒ ": np.nan
            }

    url_before = "https://www.boatrace.jp/owpc/pc/race/beforeinfo"
    res_before = session.get(url_before, params=params, timeout=15)
    if res_before.status_code == 200:
        res_before.encoding = res_before.apparent_encoding
        soup_before = BeautifulSoup(res_before.text, "html.parser")
        for bt_tbody in soup_before.select("tbody.is-fs12"):
            first_tr = bt_tbody.find("tr")
            if first_tr:
                tds = first_tr.find_all("td")
                if len(tds) >= 5:
                    b_waku = normalize_text(tds[0].get_text(strip=True))
                    if b_waku.isdigit() and int(b_waku) in racers_info:
                        t_val = tds[4].get_text(strip=True)
                        try: racers_info[int(b_waku)]["å±•ç¤ºã‚¿ã‚¤ãƒ "] = float(t_val)
                        except: pass

    if len(racers_info) < 6:
        raise ValueError("6è‰‡åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒæƒã£ã¦ã„ã¾ã›ã‚“ã€‚")
    return racers_info


# --- äºˆæ¸¬é–¢æ•° ---
def generate_predictions(hd_input, rno_input):
    racers_info = scrape_target_race_basic(hd_input, rno_input)
    clf_top3, clf_1st, clf_2nd, features = load_models()
    race_df = load_race_data()
    boatracer_df = load_and_preprocess_boatracer()

    mock_features = []
    exhibit_times = [v["å±•ç¤ºã‚¿ã‚¤ãƒ "] for v in racers_info.values() if not pd.isna(v["å±•ç¤ºã‚¿ã‚¤ãƒ "])]
    avg_exhibit = np.mean(exhibit_times) if exhibit_times else 6.80

    for c in range(1, 7):
        r_num = racers_info[c]["ç™»éŒ²ç•ªå·"]
        e_time = racers_info[c]["å±•ç¤ºã‚¿ã‚¤ãƒ "]
        if pd.isna(e_time): e_time = avg_exhibit
        
        hist = race_df[race_df['ç™»éŒ²ç•ªå·'] == r_num]
        
        row_dict = {
            'æ ç•ª': c, 'ç™»éŒ²ç•ªå·': r_num,
            'å…¨å›½å‹ç‡': racers_info[c]["å…¨å›½å‹ç‡"],
            'å…¨å›½3é€£': racers_info[c]["å…¨å›½3é€£"],
            'ãƒ¢ãƒ¼ã‚¿ãƒ¼3é€£': racers_info[c]["ãƒ¢ãƒ¼ã‚¿ãƒ¼3é€£"],
            'å±•ç¤ºã‚¿ã‚¤ãƒ _diff': e_time - avg_exhibit
        }

        if not hist.empty:
            rec = hist.iloc[-1]
            cols = ['ã‚³ãƒ¼ã‚¹1_å¹³å‡ç€é †', 'ã‚³ãƒ¼ã‚¹2_å¹³å‡ç€é †', 'ã‚³ãƒ¼ã‚¹3_å¹³å‡ç€é †', 'ã‚³ãƒ¼ã‚¹4_å¹³å‡ç€é †', 'ã‚³ãƒ¼ã‚¹5_å¹³å‡ç€é †', 'ã‚³ãƒ¼ã‚¹6_å¹³å‡ç€é †']
            vals = pd.to_numeric(rec[cols], errors='coerce').values
            
            valid_all_vals = [v for v in vals if not np.isnan(v)]
            row_dict['å…¨ã‚³ãƒ¼ã‚¹å¹³å‡ç€é †'] = np.mean(valid_all_vals) if len(valid_all_vals) > 0 else 5.0

            if c == 1: valid_vals = [v for v in vals[0:2] if not np.isnan(v)]
            elif c == 6: valid_vals = [v for v in vals[4:6] if not np.isnan(v)]
            elif 1 < c < 6: valid_vals = [v for v in vals[c-2:c+1] if not np.isnan(v)]
            row_dict['smoothed_course_rank'] = np.mean(valid_vals) if len(valid_vals) > 0 else 5.0
        else:
            row_dict['å…¨ã‚³ãƒ¼ã‚¹å¹³å‡ç€é †'] = 5.0
            row_dict['smoothed_course_rank'] = 5.0
            
        mock_features.append(row_dict)

    target_df = pd.DataFrame(mock_features)
    target_df['æ ç•ª'] = pd.to_numeric(target_df['æ ç•ª'], errors='coerce')
    target_df = pd.merge(target_df, boatracer_df, left_on=['ç™»éŒ²ç•ªå·', 'æ ç•ª'], right_on=['ç™»éŒ²ç•ªå·', 'ã‚³ãƒ¼ã‚¹'], how='left')

    target_df['3é€£å¯¾ç‡(%)'] = target_df['3é€£å¯¾ç‡(%)'].fillna(10.0)
    target_df['1ç€ç‡(%)'] = target_df['1ç€ç‡(%)'].fillna(0.0)
    target_df['2ç€ç‡(%)'] = target_df['2ç€ç‡(%)'].fillna(5.0)
    target_df['3ç€ç‡(%)'] = target_df['3ç€ç‡(%)'].fillna(5.0)

    for col in features:
        target_df[col] = pd.to_numeric(target_df[col], errors='coerce')

    X_pred = target_df[features]
    
    target_df['prob_top3'] = clf_top3.predict_proba(X_pred)[:, 1]
    target_df['prob_1st'] = clf_1st.predict_proba(X_pred)[:, 1]
    target_df['prob_2nd'] = clf_2nd.predict_proba(X_pred)[:, 1]

    excluded_boats = target_df[target_df['prob_top3'] <= 0.025]['æ ç•ª'].tolist()
    valid_df = target_df[~target_df['æ ç•ª'].isin(excluded_boats)]
    
    top_1st = valid_df.nlargest(2, 'prob_1st')['æ ç•ª'].tolist()
    top_2nd = valid_df.nlargest(3, 'prob_2nd')['æ ç•ª'].tolist()

    combinations = []
    for c1 in top_1st:
        for c2 in top_2nd:
            if c1 == c2: continue
            for c3 in valid_df['æ ç•ª'].tolist():
                if c3 == c1 or c3 == c2: continue
                p1 = target_df[target_df['æ ç•ª'] == c1]['prob_1st'].values[0]
                p2 = target_df[target_df['æ ç•ª'] == c2]['prob_2nd'].values[0]
                p3 = target_df[target_df['æ ç•ª'] == c3]['prob_top3'].values[0]
                score = p1 * p2 * p3
                combinations.append({"è²·ã„ç›®": f"{int(c1)}-{int(c2)}-{int(c3)}", "AIã‚¹ã‚³ã‚¢": round(score, 4)})

    combinations.sort(key=lambda x: x["AIã‚¹ã‚³ã‚¢"], reverse=True)
    return combinations[:30], excluded_boats


# --- UIéƒ¨åˆ† ---
with st.form("prediction_form"):
    col1, col2 = st.columns(2)
    with col1:
        hd_input = st.text_input("ğŸ“… ãƒ¬ãƒ¼ã‚¹æ—¥ç¨‹ (ä¾‹: 20260221)", value="20260221")
    with col2:
        rno_input = st.number_input("ğŸ ãƒ¬ãƒ¼ã‚¹ç•ªå· (1ã€œ12)", min_value=1, max_value=12, value=1)
    
    submitted = st.form_submit_button("AIäºˆæƒ³ã‚’å®Ÿè¡Œã™ã‚‹")

if submitted:
    with st.spinner("å‡ºèµ°è¡¨ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¨AIäºˆæƒ³ã‚’å®Ÿè¡Œä¸­..."):
        try:
            results, excluded = generate_predictions(hd_input, rno_input)
            
            st.success("äºˆæƒ³ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            
            if excluded:
                st.warning(f"âš ï¸ é™¤å¤–è‰‡ï¼ˆ3ç€ä»¥å†…ã®ç¢ºç‡2.5%ä»¥ä¸‹ï¼‰: {', '.join([str(int(x)) for x in excluded])}å·è‰‡")
            else:
                st.info("â„¹ï¸ ä»Šå›ã®ãƒ¬ãƒ¼ã‚¹ã§é™¤å¤–ã•ã‚ŒãŸè‰‡ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

            st.subheader("ğŸ† 3é€£å˜ äºˆæƒ³")
            # çµæœã‚’ç¶ºéº—ãªãƒ†ãƒ¼ãƒ–ãƒ«ã§è¡¨ç¤º
            result_df = pd.DataFrame(results)
            result_df.index = np.arange(1, len(result_df) + 1) # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’1ã‹ã‚‰ã«ã™ã‚‹
            st.dataframe(result_df, use_container_width=True)

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")